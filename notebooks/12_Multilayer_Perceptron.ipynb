{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Multilayer Perceptron**\n",
    "\n",
    "- It is a fully connected neural network\n",
    "  - Fully connected means each node of a layer is connected to each node of the next layer\n",
    "- It is also a feed-forward neural network\n",
    "  - Information goes from one side to another, from left-to-right\n",
    "  - Information doesn't flow backwards\n",
    "- That's why Multilayer Perceptron is considered fully connected feed forward neural network\n",
    "\n",
    "![Multilayer Perceptron](./images/mlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Activation function used in Output layer**\n",
    "- If there are multiple nodes in output layer\n",
    "  - Softmax activation\n",
    "- If there is only one node in output layer\n",
    "  - Sigmoid activation\n",
    "\n",
    "**It is a common practice to use softmax activation (with 2 nodes in output layer) even for binary classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Activation function used in Hidden layer**\n",
    "- Clasically, Logistic sigmoid activation function is used\n",
    "- But, ReLU is more commonly used\n",
    "$$\n",
    "ReLU(z) = \\begin{cases}\n",
    "  z, \\ \\textbf{if} \\ z \\geq 0 \\\\\n",
    "  0, \\ otherwise\n",
    "\\end{cases}\n",
    "$$\n",
    "also known as piecewise linear function\n",
    "\n",
    "Another notation is $ReLU(z) = max(0, z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Weights initialization**\n",
    "- Initialize the weights to small random numbers, instead of zeroes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
