{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Why do we normalize inputs?**\n",
    "- In a real world dataset, each feature will have a different scale\n",
    "- This poses a problem for the gradient descent algorithm\n",
    "- Recall, the formula for gradient descent is $\\textbf{w} = \\textbf{w} + \\alpha\\nabla\\mathscr{L}_{\\textbf{w}}$\n",
    "- Here, each element of $\\nabla\\mathscr{L}_{\\textbf{w}}$ is a partial derivative term\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial\\mathscr{L}}{\\partial w_{1}} \\\\[5pt]\n",
    "\\frac{\\partial\\mathscr{L}}{\\partial w_{2}} \\\\[5pt]\n",
    "... \\\\[5pt]\n",
    "\\frac{\\partial\\mathscr{L}}{\\partial w_{m}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "- And each of the above partial derivative depends on a feature input\n",
    "- If the features are on different scale, all the partial derivatives will be on a different scale\n",
    "- And if the partial derivatives are on a different scale, it will be very difficult to find a learning rate which works well for these different terms\n",
    "- Hence, we normalize the inputs such that it makes it easier to find a good learning rate\n",
    "- We'll get numerically more stable gradients\n",
    "- Training would be faster due to faster convergence (fewer epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **0-1 Normalization**\n",
    "> Also known as Min-max normalization\n",
    "$$\n",
    "x'^{[i]}_{j} = \\frac{x^{[i]}_{j} - min(x_{j})}{max(x_{j}) - min(x_{j})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Z-score standardization**\n",
    "$$\n",
    "x'^{[i]}_{[j]} = \\frac{x^{[i]}_{j} - mean(x_{j})}{std(x_{j})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardization is sometimes a bit better due to zero-centering**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
