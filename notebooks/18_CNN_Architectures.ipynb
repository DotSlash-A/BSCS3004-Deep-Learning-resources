{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Some concepts behind CNNs**\n",
    "- **Sparse-connectivity:**\n",
    "  - A single element in the feature map is connected to only a small patch of pixels\n",
    "  - This is unlike Multilayer perceptrons where all layers were fully connected\n",
    "  - Basically, neighbouring pixels are related\n",
    "- **Parameter-sharing:**\n",
    "  - Same weights are used for different patches of the input image\n",
    "  - Essentially, same filters work for different parts of the image\n",
    "- **Many layers:**\n",
    "  - Combining extracted local patterns to global patterns\n",
    "\n",
    "These assumptions and restrictions are called **Inductive biases** which help CNNs learn more quickly and generalize better (unlike fully-connected networks). We also reduce the training data which is required.\n",
    "\n",
    "Also, convolutional layers are pretty small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What goes wrong in a fully-connected Multilayer Perceptron**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.layers = torch.nn.Sequential(\n",
    "      # 1st hidden layer\n",
    "      torch.nn.Linear(3 * 224 * 224, 10000),  # Input of 224x224 size (3 colour channels) [3*224*224*10000 + 10000(bias) = 1,505,290,000]\n",
    "      torch.nn.ReLU(),\n",
    "\n",
    "      # 2nd hidden layer\n",
    "      torch.nn.Linear(10000, 1000),           # [10000 * 1000 + 1000 = 10,001,000]\n",
    "      torch.nn.ReLU(),\n",
    "\n",
    "      # 3rd hidden layer\n",
    "      torch.nn.Linear(1000, 100),             # [1000 * 100 + 100 = 100,100]\n",
    "      torch.nn.ReLU(),\n",
    "\n",
    "      # output layer\n",
    "      torch.nn.Linear(100, 10)                # [100 * 10 + 10 = 1,010]\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):                       # In total: 1,515,392,110 parameters\n",
    "    x = torch.flatten(x, start_dim = 1)\n",
    "    logits = self.layer(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 5.645GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29827/1101737098.py:7: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  size += sys.getsizeof(param.storage()) / 1024 ** 3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "size = 0\n",
    "mlp = MLP()\n",
    "\n",
    "for name, param in mlp.named_parameters():\n",
    "  size += sys.getsizeof(param.storage()) / 1024 ** 3\n",
    "\n",
    "print(f\"Model size: {size:.3f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing CNN of the above piece of code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.cnn_layers = torch.nn.Sequential(\n",
    "      torch.nn.Conv2d(3, 8, kernel_size = 5, stride = 2),   # 3 * 5 * 5 * 8 + 8 = 608\n",
    "      torch.nn.ReLU(),\n",
    "\n",
    "      torch.nn.Conv2d(8, 24, kernel_size = 5, stride = 2),  # 8 * 5 * 5 * 24 + 24 = 4,824\n",
    "      torch.nn.ReLU(),\n",
    "\n",
    "      torch.nn.Conv2d(24, 32, kernel_size = 3, stride = 2), # 24 * 3 * 3 * 32 + 32 = 6,944\n",
    "      torch.nn.ReLU(),\n",
    "\n",
    "      torch.nn.Conv2d(32, 48, kernel_size = 3, stride = 2), # 32 * 3 * 3 * 48 + 48 = 13,872\n",
    "      torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.fc_layers = torch.nn.Sequential(\n",
    "      torch.nn.Linear(48 * 12 * 12, 200),                   # 48 * 12 * 12 * 200 + 200 = 1,382,600\n",
    "      torch.nn.ReLU(),\n",
    "      torch.nn.Linear(200, 10)                              # 200 * 10 + 10 = 2,010\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):                                     # In total: 1,410,858\n",
    "    x = self.cnn_layers(x)\n",
    "    x = torch.flatten(x, start_dim = 1)\n",
    "    logits = self.fc_layers(x)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 0.005GB\n"
     ]
    }
   ],
   "source": [
    "size = 0\n",
    "cnn = CNN()\n",
    "\n",
    "for name, param in cnn.named_parameters():\n",
    "  size += sys.getsizeof(param.storage()) / 1024 ** 3\n",
    "\n",
    "print(f\"Model size: {size:.3f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We do NOT design CNN architectures from scratch, instead we adopt popular architectures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **AlexNet**\n",
    "- Won ImageNet 2012 competition\n",
    "- One of the first CNNs utilizing GPUs for efficient training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **VGG16**\n",
    "- Same basic architecture, but more layers, bigger size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GoogLeNet/Inception**\n",
    "![](../images/googlenet.png)\n",
    "- **Inception modules:** Use multiple convolution layers with smaller kernels in parallel\n",
    "- Keeps model smaller\n",
    "- Extract features at various scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/googlenet_loss.png)\n",
    "- There's auxiliary loss function in Inception\n",
    "- Total loss is basically the sum of all these losses\n",
    "- Sometimes, it's very hard to train very large neural networks\n",
    "- In back propagation, the signal sometimes degrade when we go from this output layer to the far away input layer\n",
    "- By having these auxiliary losses, we can ensure that each small sub-part of the network runs well, which then helps with overall training of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ResNet-34, 50, 101**\n",
    "- Stands for Residual Neural Networks\n",
    "- The number refers to the number of layers\n",
    "- The key idea here is \"skip connections\"\n",
    "  - It can ignore \"bad\" layers if stronger signal during backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Convolutional Vision Transformer Hybrid**\n",
    "- Transformers can also be used for vision tasks\n",
    "- It combines convolutional networks and transformers based on self-attention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
