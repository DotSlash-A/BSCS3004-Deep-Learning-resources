{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Softmax Activation function**\n",
    "\n",
    "- It converts the outputs into a probability distribution\n",
    "- It's a score with represents class membership probability for any label $t$\n",
    "- Basically, a version of Logistic Regression which can be applied to multiple (more than 2) classes\n",
    "$$\n",
    "P(y = t | z^{[i]}_{t}) = \\sigma_{softmax}(z^{[i]}_{t}) = \\frac{e^{z^{[i]}_{t}}}{\\sum^{h}_{j = 1}e^{z^{[i]}_{j}}}\n",
    "$$\n",
    "- Here, $z^{[i]}_{t}$ represents a single training example ($i-th$ training example)\n",
    "- Basically, the above equation reads \"Here is the probability that the given input belongs to class $t$ given it's features\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.tensor([[3.1, -2.3, 5.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.2955e-02, 2.8434e-04, 9.3676e-01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(z, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0.063,     0.000,     0.937]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you have scientific notation\n",
    "torch.set_printoptions(precision = 3, sci_mode = False)\n",
    "s = F.softmax(z, dim = 1)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the probability distributions sum up to 1\n",
    "torch.sum(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Now, convert these scores into class labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the index of highest probability\n",
    "torch.argmax(s, dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cross-entropy loss**\n",
    "\n",
    "- It is the loss function used in softmax regression\n",
    "- Let's start with **Binary cross-entropy loss function**, which was\n",
    "$$\n",
    "L = \\frac{1}{n}\\sum^{n}_{i = 1}-(y^{[i]}\\log(a^{[i]}) + (1 - y^{[i]})\\log(1 - a^{[i]}))\n",
    "$$\n",
    "- **Note the $\\sum$, if we have multiple training examples, we sum the loss over all training examples**\n",
    "- Also, the $a$ here refers the output of activation value (in this case, it's the output of sigmoid activation function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-category cross-entropy loss function** for multiple training examples and multiple classes\n",
    "$$\n",
    "L = \\frac{1}{n}\\sum^{n}_{i = 1}\\sum^{K}_{k = 1}-y^{[i]}_{k}\\log(a^{[i]}_{k})\n",
    "$$\n",
    "- First, we need to one-hot encode the labels for cross-entropy loss to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A complete example (kind of)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_inputs = torch.tensor([\n",
    "  [1.5, 0.1, -0.4],\n",
    "  [0.5, 0.7, 2.1],\n",
    "  [-2.1, 1.1, 0.8],\n",
    "  [1.1, 2.5, -1.2]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.716, 0.177, 0.107],\n",
       "        [0.139, 0.170, 0.690],\n",
       "        [0.023, 0.561, 0.416],\n",
       "        [0.194, 0.787, 0.019]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations = torch.softmax(net_inputs, dim = 1)\n",
    "activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [0, 1, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([0, 2, 2, 1])\n",
    "y_onehot = F.one_hot(y)\n",
    "y_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For training example $1:$\n",
    "  - Take $1^{st}$ row from softmax activations and one-hot vector\n",
    "  - Pass it through the cross-entropy loss\n",
    "$$\n",
    "-1 \\cdot \\log(0.716) - 0 \\cdot \\log(0.177) - 0 \\cdot \\log(0.107) = -\\log(0.716) \\approx 0.334\n",
    "$$\n",
    "- Do this for all the other rows, if you want lol"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
